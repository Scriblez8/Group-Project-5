{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5 - 03 Predictive model\n",
    "\n",
    "\n",
    "Utilizing the [Coronavirus Disease 2019 (COVID-19) Clinical Data Repository](https://covidclinicaldata.org/), we will examine the influence of individual symptoms on whether a COVID test will be positive. We hope that the understanding gained will help frontline medical works with limited time and resources to triage cases and determine initial steps in creating treatment plans.\n",
    "\n",
    "\n",
    "We have 3 notebooks as blow:\n",
    "* 01, 02 - Designed to investigate association between COVID and the patients' symptoms\n",
    "* 03 - Designed to create a classification model which predicts whether someone has COVID\n",
    "\n",
    "This is 3rd notebook, and we want to find as accurate model as possible\n",
    "\n",
    "The dataset has `covid19_test_results`, which says positive or negative based on patients' test results so we set this column as target value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble DataFrame and cleaning\n",
    "\n",
    "We'll want all the individual `.csv` files in one data frame. Most of the files contain the Carbon Health testing data for one week with the exception of the first file which contains data for one month. Fortunately, the compiler of this data has maintained consistency in the features and data logging for the project, so we should be able to jump right into cleaning.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get entire data\n",
    "df = pd.concat([pd.read_csv(f'../data/original_data/{file}') for file in os.listdir('../data/original_data/')], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93995, 46)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "batch_date                           0\n",
       "test_name                            0\n",
       "swab_type                            0\n",
       "covid19_test_results                 0\n",
       "age                                  0\n",
       "high_risk_exposure_occupation      169\n",
       "high_risk_interactions           24827\n",
       "diabetes                             0\n",
       "chd                                  0\n",
       "htn                                  0\n",
       "cancer                               0\n",
       "asthma                               0\n",
       "copd                                 0\n",
       "autoimmune_dis                       0\n",
       "smoker                               0\n",
       "temperature                      46453\n",
       "pulse                            45716\n",
       "sys                              47472\n",
       "dia                              47472\n",
       "rr                               52547\n",
       "sats                             46460\n",
       "rapid_flu_results                93741\n",
       "rapid_strep_results              93604\n",
       "ctab                             58528\n",
       "labored_respiration              45248\n",
       "rhonchi                          70651\n",
       "wheezes                          66507\n",
       "days_since_symptom_onset         78130\n",
       "cough                               15\n",
       "cough_severity                   88284\n",
       "fever                            22921\n",
       "sob                                206\n",
       "sob_severity                     91159\n",
       "diarrhea                           187\n",
       "fatigue                            176\n",
       "headache                           183\n",
       "loss_of_smell                      190\n",
       "loss_of_taste                      190\n",
       "runny_nose                         187\n",
       "muscle_sore                        182\n",
       "sore_throat                        183\n",
       "cxr_findings                     93306\n",
       "cxr_impression                   93306\n",
       "cxr_label                        93306\n",
       "cxr_link                         93306\n",
       "er_referral                      82826\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entire DataFrame shapes 94684 rows and 47 columns. There are some columns which have over 90000 columns. This is too many to keep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns which has more than 90000 missing values\n",
    "# 'rapid_flu_results', 'rapid_strep_results', 'sob_severity', 'cxr_findings', 'cxr_impression', 'cxr_label', 'cxr_link']\n",
    "df.drop(df.columns[df.isnull().sum()>90000], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "There are several columns which can be converted to numeric, and we will binarize all of booleans.\n",
    "Some object columns, `batch_date`, `test_name`, `swab_type`, are left but we believe these features will not help prediction accuracy so that we will drop it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize booleans through entire DataFrame\n",
    "df = df.replace(True, 1).replace(False, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cough_severity to numeric\n",
    "df = df.replace('Mild', 1).replace('Moderate', 2).replace('Severe', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test result to numeric\n",
    "df['covid19_test_results'] = df['covid19_test_results'].map({'Positive': 1, 'Negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumify and make Dataframe which contains only object columns.\n",
    "# These columns look not important but I keep it just in case\n",
    "dummy = pd.get_dummies(data=df, columns=['test_name', 'swab_type'])\n",
    "dummy['batch_date'] = pd.to_datetime(df['batch_date'])\n",
    "df.drop(columns={'batch_date', 'test_name', 'swab_type'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.986031\n",
       "1    0.013969\n",
       "Name: covid19_test_results, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['covid19_test_results'].value_counts(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We found this dataset is very imbalanced as there are many negative results compared to positive results so we will conduct oversample from positive rows and reduce negative rows**\n",
    "\n",
    "We will make DataFrame for each of positive result and negative result, then process EDA on each DataFrame separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe for positives and negatives\n",
    "positive = df[df['covid19_test_results']==1]\n",
    "negative = df[df['covid19_test_results']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92682, 36)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1313, 36)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "covid19_test_results                 0\n",
       "age                                  0\n",
       "high_risk_exposure_occupation      164\n",
       "high_risk_interactions           24592\n",
       "diabetes                             0\n",
       "chd                                  0\n",
       "htn                                  0\n",
       "cancer                               0\n",
       "asthma                               0\n",
       "copd                                 0\n",
       "autoimmune_dis                       0\n",
       "smoker                               0\n",
       "temperature                      46211\n",
       "pulse                            45485\n",
       "sys                              47223\n",
       "dia                              47223\n",
       "rr                               52202\n",
       "sats                             46223\n",
       "ctab                             58100\n",
       "labored_respiration              45012\n",
       "rhonchi                          69929\n",
       "wheezes                          65884\n",
       "days_since_symptom_onset         77487\n",
       "cough                               15\n",
       "cough_severity                   87347\n",
       "fever                            22714\n",
       "sob                                201\n",
       "diarrhea                           182\n",
       "fatigue                            171\n",
       "headache                           178\n",
       "loss_of_smell                      185\n",
       "loss_of_taste                      185\n",
       "runny_nose                         182\n",
       "muscle_sore                        177\n",
       "sore_throat                        178\n",
       "er_referral                      81828\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean `negative`\n",
    "Negative DataFrame is much huger than positive DataFrame. In order to scale `negative` to same size as `positive`, We will drop rows which have missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nao\\.conda\\envs\\dsi\\lib\\site-packages\\pandas\\core\\frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "# We will drop and decrease negative rows:\n",
    "# Drop rows which has a missing value in any columns which have less than 80000 missing values\n",
    "# The reason of 80000 is to avoid to decrease DataFrame size too much\n",
    "for column in negative.columns[negative.isnull().sum()<80000]:\n",
    "    negative.drop(negative[column][negative[column].isnull()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5322, 36)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cough_severity    3453\n",
       "er_referral       4365\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing values of negative\n",
    "negative.isnull().sum()[negative.isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 columns still have missing values.\n",
    "1. We will drop `er_refferal` as it has too many missing values.\n",
    "1. We drop rows which have missing values in `cough_severity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nao\\.conda\\envs\\dsi\\lib\\site-packages\\pandas\\core\\frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "# Drop er_referral as too many missing value\n",
    "negative.drop(columns={'er_referral'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows which has missing value at cough_severity\n",
    "negative.drop(negative['cough_severity'][negative['cough_severity'].isnull()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1869, 35)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1313, 36)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean `positive`\n",
    "`positive` is small DataFrame so we do not want to drop many rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "high_risk_exposure_occupation      5\n",
       "high_risk_interactions           235\n",
       "temperature                      242\n",
       "pulse                            231\n",
       "sys                              249\n",
       "dia                              249\n",
       "rr                               345\n",
       "sats                             237\n",
       "ctab                             428\n",
       "labored_respiration              236\n",
       "rhonchi                          722\n",
       "wheezes                          623\n",
       "days_since_symptom_onset         643\n",
       "cough_severity                   937\n",
       "fever                            207\n",
       "sob                                5\n",
       "diarrhea                           5\n",
       "fatigue                            5\n",
       "headache                           5\n",
       "loss_of_smell                      5\n",
       "loss_of_taste                      5\n",
       "runny_nose                         5\n",
       "muscle_sore                        5\n",
       "sore_throat                        5\n",
       "er_referral                      998\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing value for positive\n",
    "positive.isnull().sum()[positive.isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **We will drop `er_refferal` as we did on `negative`**\n",
    "1. **There are many columns which have only 5 missing values. This is small enough to drop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nao\\.conda\\envs\\dsi\\lib\\site-packages\\pandas\\core\\frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "# Drop er_referral as we dropped this column from negative\n",
    "positive.drop(columns={'er_referral'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows which has missing value at below columns which has 5 missing values\n",
    "# 'high_risk_exposure_occupation', 'sob', 'diarrhea', 'fatigue', 'headache', 'loss_of_smell', 'loss_of_taste', 'runny_nose', 'muscle_sore', 'sore_throat'\n",
    "positive.drop(positive['sob'][positive['sob'].isnull()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "high_risk_interactions      230\n",
       "temperature                 238\n",
       "pulse                       227\n",
       "sys                         245\n",
       "dia                         245\n",
       "rr                          341\n",
       "sats                        233\n",
       "ctab                        423\n",
       "labored_respiration         235\n",
       "rhonchi                     717\n",
       "wheezes                     618\n",
       "days_since_symptom_onset    638\n",
       "cough_severity              932\n",
       "fever                       202\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing value for positive\n",
    "positive.isnull().sum()[positive.isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1308, 35)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "positive.reset_index(drop=True, inplace=True)\n",
    "negative.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on `positive`\n",
    "We will fill out missing values for `positive` with 2 methods.\n",
    "* For columns which have only 1 or 0, we will randomly generate binomial values with probability based on each column\n",
    "    * If portion of 1 in a column of `positive` is 20 %, generate binomial values with 20% probability and fill the column with the values\n",
    "* For other columns, we will randomly generate normally distributed values with mean and std based each column\n",
    "    * If a columns of `positive` has mean of 10 and standard deviation of 3, generate noramlly distributed values with the mean and std\n",
    "    * This would not be very good for all of columns but we guess this is better than just fill those with its mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary column and continuous column\n",
    "bi_col = ['high_risk_interactions', 'ctab', 'labored_respiration', 'wheezes', 'fever', 'rhonchi']\n",
    "con_col = ['temperature', 'pulse', 'sys', 'dia', 'rr', 'sats', 'days_since_symptom_onset', 'cough_severity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fill missing values of binary columns for positive\n",
    "# Fill 1 at percentage of 1 which are given by other values in same column\n",
    "def fill_binary(column):\n",
    "    missing_index = positive[column][positive[column].isnull()].index # Index which has missing value at 'column'\n",
    "    prob = positive[column].value_counts(1).iloc[1] # Get portion of normalized 1\n",
    "    fil = np.random.binomial(1, prob, len(missing_index)) # Binomial list at probability of prob\n",
    "    for index, value in zip(missing_index, fil):\n",
    "        positive.loc[index, column] = value # Fill missing values with above binomial list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same above but for continuous columns\n",
    "# Filling values are given by normalized distribution with mean and std of each column\n",
    "def fill_con(column):\n",
    "    missing_index = positive[column][positive[column].isnull()].index\n",
    "    prob = positive[column].value_counts(1).iloc[1]\n",
    "    fil = np.random.normal(loc=positive.describe()[column].loc['mean'], scale=positive.describe()[column].loc['std'], size=len(missing_index))\n",
    "    for index, value in zip(missing_index, fil):\n",
    "        positive.loc[index, column] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nao\\.conda\\envs\\dsi\\lib\\site-packages\\pandas\\core\\indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values for binary columns\n",
    "for col in bi_col:\n",
    "    fill_binary(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values for continuouse columns\n",
    "for col in con_col:\n",
    "    fill_con(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample positive\n",
    "`positive` is still small compared to `negative` so that we will resample `positive`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_lst = []\n",
    "positive_lst.append(positive.sample(positive.shape[0], replace=True))\n",
    "positive_lst.append(positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2616, 35)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resampled positive DataFrame\n",
    "boost_positive = pd.concat(positive_lst)\n",
    "boost_positive.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "We will create following models\n",
    "* LogisticRegrssion\n",
    "* Neural network\n",
    "* SVM\n",
    "* VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinate positive and negative\n",
    "temp = pd.concat([boost_positive, negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = temp.drop('covid19_test_results', axis=1)\n",
    "y = temp['covid19_test_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4485, 34)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.583278\n",
       "0    0.416722\n",
       "Name: covid19_test_results, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline\n",
    "y.value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "Z_train = ss.fit_transform(X_train)\n",
    "Z_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(), n_jobs=6,\n",
       "             param_grid={'C': array([0.        , 0.26315789, 0.52631579, 0.78947368, 1.05263158,\n",
       "       1.31578947, 1.57894737, 1.84210526, 2.10526316, 2.36842105,\n",
       "       2.63157895, 2.89473684, 3.15789474, 3.42105263, 3.68421053,\n",
       "       3.94736842, 4.21052632, 4.47368421, 4.73684211, 5.        ]),\n",
       "                         'degree': [1, 2, 3, 4],\n",
       "                         'kernel': ['linear', 'rbf', 'polynomial', 'sigmoid']})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC()\n",
    "\n",
    "p_grid = {\n",
    "    'C':np.linspace(0, 5, 20),\n",
    "    'kernel':['linear', 'rbf', 'polynomial','sigmoid'],\n",
    "    'degree':[1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "gssvc = GridSearchCV(estimator=svc, param_grid=p_grid, cv=5, n_jobs=6)\n",
    "gssvc.fit(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=3.1578947368421053, degree=1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_svm = gssvc.best_estimator_\n",
    "best_svm.fit(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.9271483794231341\n",
      "test 0.8832442067736186\n"
     ]
    }
   ],
   "source": [
    "print('train', best_svm.score(Z_train, y_train))\n",
    "print('test', best_svm.score(Z_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(Z_train.shape[1],)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['acc'])\n",
    "results = model.fit(Z_train, y_train, epochs=30, verbose=0, batch_size=256, validation_data=(Z_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 471us/step - loss: 0.0990 - acc: 0.8672\n",
      "[0.09897058457136154, 0.8672014474868774]\n",
      "train 0.9030627608299255\n",
      "test 0.8672014474868774\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(Z_test, y_test))\n",
    "print('train', results.history['acc'][-1])\n",
    "print('test', results.history['val_acc'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.8218852215283973\n",
      "test 0.8315508021390374\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=5000)\n",
    "lr.fit(X_train, y_train)\n",
    "print('train', lr.score(X_train, y_train))\n",
    "print('test', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=VotingClassifier(estimators=[('ada',\n",
       "                                                     AdaBoostClassifier()),\n",
       "                                                    ('gb',\n",
       "                                                     GradientBoostingClassifier()),\n",
       "                                                    ('dt',\n",
       "                                                     DecisionTreeClassifier()),\n",
       "                                                    ('knn_pipe',\n",
       "                                                     Pipeline(steps=[('ss',\n",
       "                                                                      StandardScaler()),\n",
       "                                                                     ('knn',\n",
       "                                                                      KNeighborsClassifier())])),\n",
       "                                                    ('xgb',\n",
       "                                                     XGBClassifier(base_score=None,\n",
       "                                                                   booster=None,\n",
       "                                                                   colsample_bylevel=None,\n",
       "                                                                   colsample_bynode=None,\n",
       "                                                                   colsample_bytre...\n",
       "                                                                   reg_alpha=None,\n",
       "                                                                   reg_lambda=None,\n",
       "                                                                   scale_pos_weight=None,\n",
       "                                                                   subsample=None,\n",
       "                                                                   tree_method=None,\n",
       "                                                                   validate_parameters=None,\n",
       "                                                                   verbosity=None))]),\n",
       "             n_jobs=6,\n",
       "             param_grid={'ada__base_estimator': [DecisionTreeClassifier(max_depth=2),\n",
       "                                                 None],\n",
       "                         'ada__n_estimators': [95, 100, 125],\n",
       "                         'dt__max_depth': [4, 5, 6],\n",
       "                         'gb__n_estimators': [20, 30, 50],\n",
       "                         'knn_pipe__knn__n_neighbors': [3, 4, 5],\n",
       "                         'xgb__gamma': [0, 1, 2]})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell needs long time to run\n",
    "knn_pipe = Pipeline([\n",
    "    ('ss' , StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "vote = VotingClassifier([\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('gb', GradientBoostingClassifier()),\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "    ('knn_pipe', knn_pipe),\n",
    "    ('xgb', XGBClassifier())\n",
    "])\n",
    "params = {\n",
    "    'ada__base_estimator' : [DecisionTreeClassifier(max_depth=2), None],\n",
    "    'ada__n_estimators' : [95, 100, 125],\n",
    "    'gb__n_estimators' : [20, 30, 50],\n",
    "    'dt__max_depth' : [4, 5, 6],\n",
    "    'knn_pipe__knn__n_neighbors' : [3, 4, 5],\n",
    "    'xgb__gamma': [0, 1, 2]\n",
    "}\n",
    "gs = GridSearchCV(vote, param_grid=params, cv=5, n_jobs=6)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9119865739758012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('ada',\n",
       "                              AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2),\n",
       "                                                 n_estimators=125)),\n",
       "                             ('gb',\n",
       "                              GradientBoostingClassifier(n_estimators=50)),\n",
       "                             ('dt', DecisionTreeClassifier(max_depth=6)),\n",
       "                             ('knn_pipe',\n",
       "                              Pipeline(steps=[('ss', StandardScaler()),\n",
       "                                              ('knn',\n",
       "                                               KNeighborsClassifier(n_neighbors=3))])),\n",
       "                             ('xgb',\n",
       "                              XGBClassifier(base_score=...\n",
       "                                            gpu_id=None, importance_type='gain',\n",
       "                                            interaction_constraints=None,\n",
       "                                            learning_rate=None,\n",
       "                                            max_delta_step=None, max_depth=None,\n",
       "                                            min_child_weight=None, missing=nan,\n",
       "                                            monotone_constraints=None,\n",
       "                                            n_estimators=100, n_jobs=None,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            random_state=None, reg_alpha=None,\n",
       "                                            reg_lambda=None,\n",
       "                                            scale_pos_weight=None,\n",
       "                                            subsample=None, tree_method=None,\n",
       "                                            validate_parameters=None,\n",
       "                                            verbosity=None))])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gs.best_score_)\n",
    "best_estimator = gs.best_estimator_\n",
    "best_estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.975319655069878\n",
      "test 0.93048128342246\n"
     ]
    }
   ],
   "source": [
    "# Score\n",
    "print('train', best_estimator.score(X_train, y_train))\n",
    "print('test', best_estimator.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make DataFrame to store scores of each estimator\n",
    "scores = {}\n",
    "for name, estimator in best_estimator.named_estimators_.items():\n",
    "    scores[name+'_train'] = estimator.score(X_train, y_train)\n",
    "    scores[name+'_test'] = estimator.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ada_train</th>\n",
       "      <th>ada_test</th>\n",
       "      <th>gb_train</th>\n",
       "      <th>gb_test</th>\n",
       "      <th>dt_train</th>\n",
       "      <th>dt_test</th>\n",
       "      <th>knn_pipe_train</th>\n",
       "      <th>knn_pipe_test</th>\n",
       "      <th>xgb_train</th>\n",
       "      <th>xgb_test</th>\n",
       "      <th>voting_train</th>\n",
       "      <th>voting_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.977401</td>\n",
       "      <td>0.926025</td>\n",
       "      <td>0.883437</td>\n",
       "      <td>0.871658</td>\n",
       "      <td>0.868272</td>\n",
       "      <td>0.849376</td>\n",
       "      <td>0.90455</td>\n",
       "      <td>0.826203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.934938</td>\n",
       "      <td>0.97532</td>\n",
       "      <td>0.930481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ada_train  ada_test  gb_train   gb_test  dt_train   dt_test  \\\n",
       "1   0.977401  0.926025  0.883437  0.871658  0.868272  0.849376   \n",
       "\n",
       "   knn_pipe_train  knn_pipe_test  xgb_train  xgb_test  voting_train  \\\n",
       "1         0.90455       0.826203        1.0  0.934938       0.97532   \n",
       "\n",
       "   voting_test  \n",
       "1     0.930481  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['voting_train'] = best_estimator.score(X_train, y_train)\n",
    "scores['voting_test'] = best_estimator.score(X_test, y_test)\n",
    "models = {}\n",
    "\n",
    "models['1'] = scores\n",
    "pd.DataFrame.from_dict(models, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclustion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best model is VotingClassifer\n",
    "\n",
    "> * Train accuracy **97%**\n",
    "> * Test accuracy **93%**\n",
    "\n",
    "|VotingClassifier parameter|\n",
    "|-|\n",
    "|**AdaBoostClassifier**(base_estimator=**DecisionTreeClassifier(max_depth=2)**, n_estimators=125)|\n",
    "|**GradientBoostingClassifier**(n_estimators=50)|\n",
    "|**DecisionTreeClassifier**(max_depth=6)|\n",
    "|**Pipeline**(steps=[('ss', StandardScaler()), ('knn', **KNeighborsClassifier(n_neighbors=3)**)])|\n",
    "|**XGBClassifier**|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation and next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model predicts at 93% accuracy so that we can reccomend frontline medical workers to use this model for brief screening of the potential patients. They would prioritize the patiets who got positive from this model to take diagnosis more carefully. Also they can group the patients based on the result whether positive or negative so that they could reduce infection onsite.\n",
    "\n",
    "Although our predictive models are very accurate, they probably aren’t accurate enough for  the medical field or a possible life and death determination.\n",
    "\n",
    "\n",
    "In order to obtain better model, we could try other combination of classifiers. VotingClassifier needs much time to calculate so that we did not try out many various of hyperparemeters but if we had more time or better machine, we would figure out better heperparameters and classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
